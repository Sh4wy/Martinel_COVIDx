{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COVIDX_keras_Breastnet_stratifiedshuffle.ipynb","provenance":[],"authorship_tag":"ABX9TyPWAW6JbHhtsgEtTbF3UhlA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lxIgX8Uw_VYo","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","print('tf version : ' + tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tTRTskrq_bYM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":128},"executionInfo":{"status":"ok","timestamp":1598253521839,"user_tz":-120,"elapsed":28208,"user":{"displayName":"Anthonin Martinel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBJB4KmVJV6_nve0Nj0v2SgLC3JJvZj-S5sD5B=s64","userId":"08903790011573102129"}},"outputId":"56f57692-d233-4b73-af7d-9db79121d728"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qVO2VMZA_dOv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1598253849008,"user_tz":-120,"elapsed":355361,"user":{"displayName":"Anthonin Martinel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBJB4KmVJV6_nve0Nj0v2SgLC3JJvZj-S5sD5B=s64","userId":"08903790011573102129"}},"outputId":"9740b7b6-885d-4d4a-9ff3-046e513aeebd"},"source":["# Unzip CovidX-v4 dataset\n","\n","# unzip Dr Arganda-Carreras's file:\n","# !unzip -q '/content/drive/My Drive/Colab Notebooks/Internship/COVID19_DATA/Covid-X-v4.zip'\n","\n","# unzip mine (a copy of Dr Arganda-Carreras's file)\n","!unzip -q '/content/drive/My Drive/Colab Notebooks/Internship/COVIDX_models/Covid-X-v4.zip'\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Done!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bphy2Dxa06Ej","colab_type":"text"},"source":["# Nouvelle section"]},{"cell_type":"code","metadata":{"id":"XBu_ETHo0kJ4","colab_type":"code","colab":{}},"source":["from keras.models import Sequential, save_model, load_model\n","from keras import regularizers, optimizers, Model\n","\n","from keras.optimizers import Adam, SGD\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n","from keras.callbacks import *\n","\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","import logging\n","\n","logging.getLogger('tensorflow').setLevel(logging.ERROR)\n","\n","\n","class CosineAnnealer:\n","    \n","    def __init__(self, start, end, steps):\n","        self.start = start\n","        self.end = end\n","        self.steps = steps\n","        self.n = 0\n","        \n","    def step(self):\n","        self.n += 1\n","        cos = np.cos(np.pi * (self.n / self.steps)) + 1\n","        return self.end + (self.start - self.end) / 2. * cos\n","\n","\n","class OneCycleScheduler(Callback):\n","    \"\"\"`Callback` that schedules the learning rate on a 1cycle policy as per Leslie Smith's paper(https://arxiv.org/pdf/1803.09820.pdf).\n","    If the model supports a momentum parameter, it will also be adapted by the schedule.\n","    The implementation adopts additional improvements as per the fastai library: https://docs.fast.ai/callbacks.one_cycle.html, where\n","    only two phases are used and the adaptation is done using cosine annealing.\n","    In phase 1 the LR increases from `lr_max / div_factor` to `lr_max` and momentum decreases from `mom_max` to `mom_min`.\n","    In the second phase the LR decreases from `lr_max` to `lr_max / (div_factor * 1e4)` and momemtum from `mom_max` to `mom_min`.\n","    By default the phases are not of equal length, with the phase 1 percentage controlled by the parameter `phase_1_pct`.\n","    \"\"\"\n","\n","    def __init__(self, lr_max, steps, mom_min=0.85, mom_max=0.95, phase_1_pct=0.3, div_factor=25.):\n","        super(OneCycleScheduler, self).__init__()\n","        lr_min = lr_max / div_factor\n","        final_lr = lr_max / (div_factor * 1e4)\n","        phase_1_steps = steps * phase_1_pct\n","        phase_2_steps = steps - phase_1_steps\n","        \n","        self.phase_1_steps = phase_1_steps\n","        self.phase_2_steps = phase_2_steps\n","        self.phase = 0\n","        self.step = 0\n","        \n","        self.phases = [[CosineAnnealer(lr_min, lr_max, phase_1_steps), CosineAnnealer(mom_max, mom_min, phase_1_steps)], \n","                 [CosineAnnealer(lr_max, final_lr, phase_2_steps), CosineAnnealer(mom_min, mom_max, phase_2_steps)]]\n","        \n","        self.lrs = []\n","        self.moms = []\n","\n","    def on_train_begin(self, logs=None):\n","        self.phase = 0\n","        self.step = 0\n","\n","        self.set_lr(self.lr_schedule().start)\n","        self.set_momentum(self.mom_schedule().start)\n","        \n","    def on_train_batch_begin(self, batch, logs=None):\n","        self.lrs.append(self.get_lr())\n","        self.moms.append(self.get_momentum())\n","\n","    def on_train_batch_end(self, batch, logs=None):\n","        self.step += 1\n","        if self.step >= self.phase_1_steps:\n","            self.phase = 1\n","            \n","        self.set_lr(self.lr_schedule().step())\n","        self.set_momentum(self.mom_schedule().step())\n","        \n","    def get_lr(self):\n","        try:\n","            return tf.keras.backend.get_value(self.model.optimizer.lr)\n","        except AttributeError:\n","            return None\n","        \n","    def get_momentum(self):\n","        try:\n","            return tf.keras.backend.get_value(self.model.optimizer.momentum)\n","        except AttributeError:\n","            return None\n","        \n","    def set_lr(self, lr):\n","        try:\n","            tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n","        except AttributeError:\n","            pass # ignore\n","        \n","    def set_momentum(self, mom):\n","        try:\n","            tf.keras.backend.set_value(self.model.optimizer.momentum, mom)\n","        except AttributeError:\n","            pass # ignore\n","\n","    def lr_schedule(self):\n","        return self.phases[self.phase][0]\n","    \n","    def mom_schedule(self):\n","        return self.phases[self.phase][1]\n","    \n","    def plot(self):\n","        ax = plt.subplot(1, 2, 1)\n","        ax.plot(self.lrs)\n","        ax.set_title('Learning Rate')\n","        ax = plt.subplot(1, 2, 2)\n","        ax.plot(self.moms)\n","        ax.set_title('Momentum')\n","\n","\n","class LRFinder(Callback):\n","    \"\"\"`Callback` that exponentially adjusts the learning rate after each training batch between `start_lr` and\n","    `end_lr` for a maximum number of batches: `max_step`. The loss and learning rate are recorded at each step allowing\n","    visually finding a good learning rate as per https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html via\n","    the `plot` method.\n","    \"\"\"\n","\n","    def __init__(self, start_lr: float = 1e-7, end_lr: float = 10, max_steps: int = 1000, smoothing=0.9):\n","        super(LRFinder, self).__init__()\n","        self.start_lr, self.end_lr = start_lr, end_lr\n","        self.max_steps = max_steps\n","        self.smoothing = smoothing\n","        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n","        self.lrs, self.losses = [], []\n","\n","    def on_train_begin(self, logs=None):\n","        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n","        self.lrs, self.losses = [], []\n","\n","    def on_train_batch_begin(self, batch, logs=None):\n","        self.lr = self.exp_annealing(self.step)\n","        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n","\n","    def on_train_batch_end(self, batch, logs=None):\n","        logs = logs or {}\n","        loss = logs.get('loss')\n","        step = self.step\n","        if loss:\n","            self.avg_loss = self.smoothing * self.avg_loss + (1 - self.smoothing) * loss\n","            smooth_loss = self.avg_loss / (1 - self.smoothing ** (self.step + 1))\n","            self.losses.append(smooth_loss)\n","            self.lrs.append(self.lr)\n","\n","            if step == 0 or loss < self.best_loss:\n","                self.best_loss = loss\n","\n","            if smooth_loss > 4 * self.best_loss or tf.math.is_nan(smooth_loss):\n","                self.model.stop_training = True\n","\n","        if step == self.max_steps:\n","            self.model.stop_training = True\n","\n","        self.step += 1\n","\n","    def exp_annealing(self, step):\n","        return self.start_lr * (self.end_lr / self.start_lr) ** (step * 1. / self.max_steps)\n","\n","    def plot(self):\n","        fig, ax = plt.subplots(1, 1)\n","        ax.set_ylabel('Loss')\n","        ax.set_xlabel('Learning Rate (log scale)')\n","        ax.set_xscale('log')\n","        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n","        ax.plot(self.lrs, self.losses)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZISt0cnf0pSg","colab_type":"code","colab":{}},"source":["# copied from https://github.com/kobiso/CBAM-keras/blob/master/models/attention_module.py\n","# used for Breastnet model : https://github.com/Goodsea/BreastNet/blob/master/40X/BreastNet_40X.ipynb adapted for keras\n","\n","from keras.layers import *\n","from keras.optimizers import *\n","from keras.models import load_model, Model\n","\n","from keras import backend as K\n","SHAPE = (224, 224, 3)\n","from keras.layers import Input\n","\n","def cbam_block(cbam_feature, ratio=8):\n","    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n","    As described in https://arxiv.org/abs/1807.06521.\n","    \"\"\"\n","    \n","    cbam_feature = channel_attention(cbam_feature, ratio)\n","    cbam_feature = spatial_attention(cbam_feature)\n","    return cbam_feature\n","\n","def channel_attention(input_feature, ratio=8):\n","    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n","    channel = input_feature._shape[channel_axis] #change _keras_shape into _shape\n","    \n","    shared_layer_one = Dense(channel//ratio,\n","                             activation='relu',\n","                             kernel_initializer='he_normal',\n","                             use_bias=True,\n","                             bias_initializer='zeros')\n","    shared_layer_two = Dense(channel,\n","                             kernel_initializer='he_normal',\n","                             use_bias=True,\n","                             bias_initializer='zeros')\n","    \n","    avg_pool = GlobalAveragePooling2D()(input_feature)    \n","    avg_pool = Reshape((1,1,channel))(avg_pool)\n","    assert avg_pool._shape[1:] == (1,1,channel) #_keras\n","    avg_pool = shared_layer_one(avg_pool)\n","    assert avg_pool._shape[1:] == (1,1,channel//ratio) #_keras\n","    avg_pool = shared_layer_two(avg_pool)\n","    assert avg_pool._shape[1:] == (1,1,channel) #_keras\n","    \n","    max_pool = GlobalMaxPooling2D()(input_feature)\n","    max_pool = Reshape((1,1,channel))(max_pool)\n","    assert max_pool._shape[1:] == (1,1,channel) #_keras\n","    max_pool = shared_layer_one(max_pool)\n","    assert max_pool._shape[1:] == (1,1,channel//ratio) #_keras\n","    max_pool = shared_layer_two(max_pool)\n","    assert max_pool._shape[1:] == (1,1,channel) #_keras\n","    \n","    cbam_feature = Add()([avg_pool,max_pool])\n","    cbam_feature = Activation('sigmoid')(cbam_feature)\n","\n","    if K.image_data_format() == \"channels_first\":\n","        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n","    \n","    return multiply([input_feature, cbam_feature])\n","\n","def spatial_attention(input_feature):\n","    kernel_size = 7\n","    \n","    if K.image_data_format() == \"channels_first\":\n","        channel = input_feature._shape[1] #_keras\n","        cbam_feature = Permute((2,3,1))(input_feature)\n","    else:\n","        channel = input_feature._shape[-1] #_keras_\n","        cbam_feature = input_feature\n","    \n","    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n","    assert avg_pool._shape[-1] == 1 #_keras\n","    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n","    assert max_pool._shape[-1] == 1 #_keras\n","    concat = Concatenate(axis=3)([avg_pool, max_pool])\n","    assert concat._shape[-1] == 2 #_keras\n","    cbam_feature = Conv2D(filters = 1,\n","                    kernel_size=kernel_size,\n","                    strides=1,\n","                    padding='same',\n","                    activation='sigmoid',\n","                    kernel_initializer='he_normal',\n","                    use_bias=False)(concat)\t\n","    assert cbam_feature._shape[-1] == 1 #_keras\n","    \n","    if K.image_data_format() == \"channels_first\":\n","        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n","        \n","    return multiply([input_feature, cbam_feature])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1GkVzdx0siP","colab_type":"code","colab":{}},"source":["\n","# credits: https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n","\n","def recall(y_true, y_pred):\n","    \"\"\"\n","    Recall metric.\n","    \n","    Only computes a batch-wise average of recall.\n","    \n","    Computes the recall, a metric for multi-label classification of\n","    how many relevant items are selected.\n","    \"\"\"\n","    \n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","def precision(y_true, y_pred):\n","    \"\"\"Precision metric.\n","    \n","    Only computes a batch-wise average of precision.\n","    \n","    Computes the precision, a metric for multi-label classification of\n","    how many selected items are relevant.\n","    \"\"\"\n","    \n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","def f1(y_true, y_pred):\n","    precisionx = precision(y_true, y_pred)\n","    recallx = recall(y_true, y_pred)\n","    return 2*((precisionx*recallx)/(precisionx+recallx+K.epsilon()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q9clbxoo0tBv","colab_type":"code","colab":{}},"source":["# copied from https://gist.github.com/mjdietzx/5319e42637ed7ef095d430cb5c5e8c64\n","def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n","    shortcut = y\n","\n","    # down-sampling is performed with a stride of 2\n","    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n","    y = BatchNormalization()(y)\n","    y = LeakyReLU()(y)\n","\n","    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n","    y = BatchNormalization()(y)\n","\n","    # identity shortcuts used directly when the input and output are of the same dimensions\n","    if _project_shortcut or _strides != (1, 1):\n","        # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n","        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n","        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n","        shortcut = BatchNormalization()(shortcut)\n","\n","    y = add([shortcut, y])\n","    y = LeakyReLU()(y)\n","\n","    return y\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoI55DqC0vrT","colab_type":"code","colab":{}},"source":["# copied from https://github.com/Goodsea/BreastNet/blob/master/40X/BreastNet_40X.ipynb, \"create_model()\"\n","\n","# We adapt this model by adding some dropout layers to counter overfitting\n","def Breastnet_modified_model():\n","    \n","    dropRate = 0.3\n","    \n","    init = Input(SHAPE)\n","    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x1 = MaxPooling2D((2,2))(x)\n","    \n","    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = cbam_block(x)\n","    x = residual_block(x, 64)\n","    x2 = MaxPooling2D((2,2))(x)\n","    \n","    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = cbam_block(x)\n","    x = residual_block(x, 128)\n","    x3 = MaxPooling2D((2,2))(x)\n","    \n","    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n","    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n","    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n","    \n","    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n","    gap = GlobalAveragePooling2D()(hypercolumn)\n","\n","    x = Dense(256, activation=None)(gap)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","    x = Dropout(dropRate)(x) # this one was already in the original model\n","    \n","    x = Dense(256, activation=None)(x)\n","    x = BatchNormalization()(x)\n","    x = Activation('relu')(x)\n","\n","    y = Dense(3, activation='softmax')(x) # modified it for 3 classes \n","   \n","    model = Model(init, y)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yim_N8mL6mku","colab_type":"code","colab":{}},"source":["def create_model(lr, epoch, STEP_SIZE_TRAIN, STEP_SIZE_VALID):\n","\n","    base_model = Breastnet_modified_model()\n","\n","    # optimizer = tf.keras.optimizers.Adam()\n","    optimizer = tf.keras.optimizers.RMSprop()\n","\n","    # optimizer = tf.keras.optimizers.Adam(amsgrad=True)\n","    base_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics= [precision, recall, f1, 'accuracy']) #, weighted_metrics = class_weights)\n","\n","    # base_model.summary()\n","    return base_model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UH3vJb6u5PQa","colab_type":"code","colab":{}},"source":["class SGDRScheduler(Callback):\n","    '''Cosine annealing learning rate scheduler with periodic restarts.\n","    # Usage\n","        ```python\n","            schedule = SGDRScheduler(min_lr=1e-5,\n","                                     max_lr=1e-2,\n","                                     steps_per_epoch=np.ceil(epoch_size/batch_size),\n","                                     lr_decay=0.9,\n","                                     cycle_length=5,\n","                                     mult_factor=1.5)\n","            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n","        ```\n","    # Arguments\n","        min_lr: The lower bound of the learning rate range for the experiment.\n","        max_lr: The upper bound of the learning rate range for the experiment.\n","        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n","        lr_decay: Reduce the max_lr after the completion of each cycle.\n","                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n","        cycle_length: Initial number of epochs in a cycle.\n","        mult_factor: Scale epochs_to_restart after each full cycle completion.\n","    # References\n","        Blog post: jeremyjordan.me/nn-learning-rate\n","        Original paper: http://arxiv.org/abs/1608.03983\n","    '''\n","    def __init__(self,\n","                 min_lr,\n","                 max_lr,\n","                 steps_per_epoch,\n","                 lr_decay=1,\n","                 cycle_length=10,\n","                 mult_factor=2):\n","\n","        self.min_lr = min_lr\n","        self.max_lr = max_lr\n","        self.lr_decay = lr_decay\n","\n","        self.batch_since_restart = 0\n","        self.next_restart = cycle_length\n","\n","        self.steps_per_epoch = steps_per_epoch\n","\n","        self.cycle_length = cycle_length\n","        self.mult_factor = mult_factor\n","\n","        self.history = {}\n","\n","    def clr(self):\n","        '''Calculate the learning rate.'''\n","        fraction_to_restart = self.batch_since_restart / (self.steps_per_epoch * self.cycle_length)\n","        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n","        return lr\n","\n","    def on_train_begin(self, logs={}):\n","        '''Initialize the learning rate to the minimum value at the start of training.'''\n","        logs = logs or {}\n","        K.set_value(self.model.optimizer.lr, self.max_lr)\n","\n","    def on_batch_end(self, batch, logs={}):\n","        '''Record previous batch statistics and update the learning rate.'''\n","        logs = logs or {}\n","        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n","        for k, v in logs.items():\n","            self.history.setdefault(k, []).append(v)\n","\n","        self.batch_since_restart += 1\n","        K.set_value(self.model.optimizer.lr, self.clr())\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        '''Check for end of current cycle, apply restarts when necessary.'''\n","        if epoch + 1 == self.next_restart:\n","            self.batch_since_restart = 0\n","            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n","            self.next_restart += self.cycle_length\n","            self.max_lr *= self.lr_decay\n","            self.best_weights = self.model.get_weights()\n","\n","    def on_train_end(self, logs={}):\n","        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n","        self.model.set_weights(self.best_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zbyhHnf201Kx","colab_type":"text"},"source":["# data"]},{"cell_type":"code","metadata":{"id":"nNgvhbuF_ebL","colab_type":"code","colab":{}},"source":["import pathlib\n","path = pathlib.Path( '/content/data' )\n","\n","# Parameters\n","\n","# Model path\n","mod_path = '/content/drive/My Drive/Colab Notebooks/Internship/proto1'\n","\n","# batch size\n","bs = 32\n","\n","import pandas as pd\n","import numpy as np\n","import keras\n","from keras_preprocessing.image import ImageDataGenerator\n","import io\n","\n","\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","# Paths\n","path_train = pathlib.Path('/content/data/train/')\n","path_test = pathlib.Path('/content/data/test/')\n","\n","\n","# initialize the data and labels\n","data = []\n","labels = []\n","\n","traindf= pd.read_csv(path/\"train_split.txt\", dtype=str, sep=' ', header=None, names=['Image','Name','Class','Origin']) #.drop('Origin',axis=1).drop('Image',axis=1)\n","testdf=pd.read_csv(path/\"test_split.txt\", dtype=str, sep=' ', header=None, names=['Image','Name','Class', 'Origin']) #.drop('Origin',axis=1).drop('Image',axis=1)\n","\n","traindf = traindf[['Name', 'Class']]\n","testdf = testdf[['Name','Class']]\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEBFI0Pt7_dT","colab_type":"code","colab":{}},"source":["from collections import Counter\n","seed = 2019\n","\n","cv = StratifiedShuffleSplit(n_splits=2, test_size=0.1, random_state=seed)\n","train_idx, val_idx = next(cv.split(traindf['Name'],traindf['Class']))\n","\n","traindfb = traindf.iloc[train_idx, :]\n","validdf = traindf.iloc[val_idx, :]\n","\n","traindf = traindfb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cUpYhjy2wbBH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"95043770-398d-498b-cd51-8e7f1be41c06"},"source":["# datagen = ImageDataGenerator(rescale=1./255,\n","#                                 rotation_range=30,\n","# \t                              zoom_range=0.15,\n","# \t                              width_shift_range=0.2,\n","# \t                              height_shift_range=0.2,\n","# \t                              shear_range=0.15,\n","# \t                              horizontal_flip=True,\n","# \t                              fill_mode=\"nearest\")\n","\n","datagen = ImageDataGenerator(rescale=1./255)\n","\n","\n","size = 224 # Size of the images\n","\n","train_generator=datagen.flow_from_dataframe(\n","      dataframe=traindf,\n","      directory=path_train,\n","      x_col='Name',\n","      y_col='Class',\n","      batch_size=bs,\n","      seed=2019,\n","      shuffle=True,\n","      class_mode=\"categorical\",\n","      target_size=(size,size))\n","\n","valid_generator=datagen.flow_from_dataframe(\n","      dataframe=validdf,\n","      directory=path_train,\n","      x_col='Name',\n","      y_col='Class',\n","      batch_size=bs,\n","      seed=2019,\n","      shuffle=True,\n","      class_mode=\"categorical\",\n","      target_size=(size,size))\n","\n","# CLR parameters\n","lr = 1e-3\n","epochs = 60\n","\n","STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n","STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n","\n","# compute class weight:\n","labels_count = Counter()\n","\n","for word in traindf['Class']:\n","    labels_count[word] += 1\n","\n","total_count = sum(labels_count.values())\n","class_weights = {cls: total_count / count for cls, count in labels_count.items()}\n","\n","class_weights[train_generator.class_indices['COVID-19']] = class_weights.pop('COVID-19')\n","class_weights[train_generator.class_indices['normal']] = class_weights.pop('normal')\n","class_weights[train_generator.class_indices['pneumonia']] = class_weights.pop('pneumonia')\n","\n","\n","model_name = 'breastnet_strat.hdf5'\n","    \n","base_model = create_model(lr, epochs, STEP_SIZE_TRAIN, STEP_SIZE_VALID)\n","\n","checkpoint_callback = ModelCheckpoint(\n","      filepath=model_name,\n","      monitor='val_accuracy', \n","      mode='max', \n","      verbose=1, \n","      save_best_only=True, \n","      save_weights_only=False)\n","\n","steps = STEP_SIZE_TRAIN/epochs\n","\n","lr_schedule = SGDRScheduler(min_lr=lr/1000,\n","                             max_lr=lr,\n","                             steps_per_epoch=np.ceil(epochs/bs),\n","                             lr_decay=0.9,\n","                             cycle_length=10,\n","                             mult_factor=2.)\n","\n","\n","\n","# lr_finder = LRFinder(max_steps=500)\n","base_model.load_weights(model_name)\n","\n","History = base_model.fit_generator(generator=train_generator,\n","                    steps_per_epoch=STEP_SIZE_TRAIN,\n","                    validation_data=valid_generator,\n","                    validation_steps=STEP_SIZE_VALID,\n","                    class_weight=class_weights, # We change the weights\n","                    epochs=epochs,\n","                    callbacks=[checkpoint_callback, lr_schedule], # checkpoint_callback, lr_schedule\n","                    workers=4,\n","                    verbose = 2)\n","\n","# import matplotlib.pyplot as plt\n","\n","# lr_finder.plot()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 12502 validated image filenames belonging to 3 classes.\n","Found 1390 validated image filenames belonging to 3 classes.\n","Epoch 1/100\n","\n","Epoch 00001: val_accuracy improved from -inf to 0.34738, saving model to breastnet_strat.hdf5\n","390/390 - 278s - loss: 2.9402 - precision: 0.6252 - recall: 0.4974 - f1: 0.5525 - accuracy: 0.5812 - val_loss: 1.2263 - val_precision: 0.3454 - val_recall: 0.3372 - val_f1: 0.3412 - val_accuracy: 0.3474\n","Epoch 2/100\n","\n","Epoch 00002: val_accuracy improved from 0.34738 to 0.64898, saving model to breastnet_strat.hdf5\n","390/390 - 275s - loss: 2.4761 - precision: 0.7037 - recall: 0.5951 - f1: 0.6439 - accuracy: 0.6638 - val_loss: 0.7958 - val_precision: 0.6499 - val_recall: 0.6439 - val_f1: 0.6468 - val_accuracy: 0.6490\n","Epoch 3/100\n","\n","Epoch 00003: val_accuracy improved from 0.64898 to 0.72238, saving model to breastnet_strat.hdf5\n","390/390 - 274s - loss: 2.1334 - precision: 0.7432 - recall: 0.6546 - f1: 0.6953 - accuracy: 0.7074 - val_loss: 0.6784 - val_precision: 0.7519 - val_recall: 0.6773 - val_f1: 0.7122 - val_accuracy: 0.7224\n","Epoch 4/100\n","\n","Epoch 00004: val_accuracy improved from 0.72238 to 0.77689, saving model to breastnet_strat.hdf5\n","390/390 - 273s - loss: 1.9589 - precision: 0.7561 - recall: 0.6790 - f1: 0.7147 - accuracy: 0.7286 - val_loss: 0.5531 - val_precision: 0.7826 - val_recall: 0.7696 - val_f1: 0.7760 - val_accuracy: 0.7769\n","Epoch 5/100\n","\n","Epoch 00005: val_accuracy did not improve from 0.77689\n","390/390 - 272s - loss: 1.8348 - precision: 0.7731 - recall: 0.6980 - f1: 0.7330 - accuracy: 0.7421 - val_loss: 2.2273 - val_precision: 0.3685 - val_recall: 0.3474 - val_f1: 0.3574 - val_accuracy: 0.3641\n","Epoch 6/100\n","\n","Epoch 00006: val_accuracy did not improve from 0.77689\n","390/390 - 273s - loss: 1.6738 - precision: 0.7778 - recall: 0.7214 - f1: 0.7480 - accuracy: 0.7529 - val_loss: 0.7399 - val_precision: 0.7319 - val_recall: 0.7304 - val_f1: 0.7311 - val_accuracy: 0.7304\n","Epoch 7/100\n","\n","Epoch 00007: val_accuracy did not improve from 0.77689\n","390/390 - 272s - loss: 1.5885 - precision: 0.8017 - recall: 0.7516 - f1: 0.7754 - accuracy: 0.7814 - val_loss: 1.3337 - val_precision: 0.5230 - val_recall: 0.4629 - val_f1: 0.4906 - val_accuracy: 0.5153\n","Epoch 8/100\n","\n","Epoch 00008: val_accuracy did not improve from 0.77689\n","390/390 - 273s - loss: 1.4211 - precision: 0.8081 - recall: 0.7634 - f1: 0.7847 - accuracy: 0.7900 - val_loss: 0.7091 - val_precision: 0.7100 - val_recall: 0.6359 - val_f1: 0.6704 - val_accuracy: 0.6868\n","Epoch 9/100\n","\n","Epoch 00009: val_accuracy did not improve from 0.77689\n","390/390 - 272s - loss: 1.4566 - precision: 0.8143 - recall: 0.7797 - f1: 0.7963 - accuracy: 0.7999 - val_loss: 2.2892 - val_precision: 0.3656 - val_recall: 0.3394 - val_f1: 0.3517 - val_accuracy: 0.3699\n","Epoch 10/100\n","\n","Epoch 00010: val_accuracy improved from 0.77689 to 0.81541, saving model to breastnet_strat.hdf5\n","390/390 - 274s - loss: 1.3426 - precision: 0.8180 - recall: 0.7815 - f1: 0.7990 - accuracy: 0.8015 - val_loss: 0.4495 - val_precision: 0.8183 - val_recall: 0.8067 - val_f1: 0.8124 - val_accuracy: 0.8154\n","Epoch 11/100\n","\n","Epoch 00011: val_accuracy did not improve from 0.81541\n","390/390 - 272s - loss: 1.2931 - precision: 0.8240 - recall: 0.7925 - f1: 0.8077 - accuracy: 0.8107 - val_loss: 0.6338 - val_precision: 0.7326 - val_recall: 0.7129 - val_f1: 0.7225 - val_accuracy: 0.7275\n","Epoch 12/100\n","\n","Epoch 00012: val_accuracy did not improve from 0.81541\n","390/390 - 274s - loss: 1.2054 - precision: 0.8339 - recall: 0.8025 - f1: 0.8176 - accuracy: 0.8201 - val_loss: 0.8162 - val_precision: 0.6963 - val_recall: 0.6926 - val_f1: 0.6944 - val_accuracy: 0.6940\n","Epoch 13/100\n","\n","Epoch 00013: val_accuracy did not improve from 0.81541\n","390/390 - 272s - loss: 1.2090 - precision: 0.8327 - recall: 0.8087 - f1: 0.8203 - accuracy: 0.8204 - val_loss: 0.7798 - val_precision: 0.6346 - val_recall: 0.5916 - val_f1: 0.6120 - val_accuracy: 0.6148\n","Epoch 14/100\n","\n","Epoch 00014: val_accuracy improved from 0.81541 to 0.82994, saving model to breastnet_strat.hdf5\n","390/390 - 274s - loss: 1.1068 - precision: 0.8368 - recall: 0.8204 - f1: 0.8284 - accuracy: 0.8292 - val_loss: 0.4046 - val_precision: 0.8320 - val_recall: 0.8241 - val_f1: 0.8280 - val_accuracy: 0.8299\n","Epoch 15/100\n","\n","Epoch 00015: val_accuracy did not improve from 0.82994\n","390/390 - 272s - loss: 1.1913 - precision: 0.8403 - recall: 0.8199 - f1: 0.8298 - accuracy: 0.8315 - val_loss: 0.7292 - val_precision: 0.7093 - val_recall: 0.6635 - val_f1: 0.6854 - val_accuracy: 0.6948\n","Epoch 16/100\n","\n","Epoch 00016: val_accuracy improved from 0.82994 to 0.83430, saving model to breastnet_strat.hdf5\n","390/390 - 274s - loss: 1.0546 - precision: 0.8458 - recall: 0.8245 - f1: 0.8349 - accuracy: 0.8372 - val_loss: 0.3890 - val_precision: 0.8367 - val_recall: 0.8263 - val_f1: 0.8314 - val_accuracy: 0.8343\n","Epoch 17/100\n","\n","Epoch 00017: val_accuracy did not improve from 0.83430\n","390/390 - 273s - loss: 1.1284 - precision: 0.8451 - recall: 0.8263 - f1: 0.8354 - accuracy: 0.8380 - val_loss: 0.5525 - val_precision: 0.7905 - val_recall: 0.7740 - val_f1: 0.7820 - val_accuracy: 0.7849\n","Epoch 18/100\n","\n","Epoch 00018: val_accuracy did not improve from 0.83430\n","390/390 - 273s - loss: 1.0180 - precision: 0.8502 - recall: 0.8315 - f1: 0.8406 - accuracy: 0.8415 - val_loss: 1.5270 - val_precision: 0.4547 - val_recall: 0.4128 - val_f1: 0.4325 - val_accuracy: 0.4440\n","Epoch 19/100\n","\n","Epoch 00019: val_accuracy did not improve from 0.83430\n","390/390 - 271s - loss: 1.0355 - precision: 0.8509 - recall: 0.8341 - f1: 0.8423 - accuracy: 0.8423 - val_loss: 0.7732 - val_precision: 0.6258 - val_recall: 0.5923 - val_f1: 0.6084 - val_accuracy: 0.6112\n","Epoch 20/100\n","\n","Epoch 00020: val_accuracy did not improve from 0.83430\n","390/390 - 273s - loss: 1.0391 - precision: 0.8537 - recall: 0.8383 - f1: 0.8458 - accuracy: 0.8463 - val_loss: 1.1361 - val_precision: 0.5500 - val_recall: 0.5124 - val_f1: 0.5302 - val_accuracy: 0.5342\n","Epoch 21/100\n","\n","Epoch 00021: val_accuracy did not improve from 0.83430\n","390/390 - 273s - loss: 1.0352 - precision: 0.8563 - recall: 0.8400 - f1: 0.8479 - accuracy: 0.8492 - val_loss: 0.6320 - val_precision: 0.7432 - val_recall: 0.7289 - val_f1: 0.7359 - val_accuracy: 0.7369\n","Epoch 22/100\n","\n","Epoch 00022: val_accuracy did not improve from 0.83430\n","390/390 - 273s - loss: 1.0330 - precision: 0.8491 - recall: 0.8364 - f1: 0.8426 - accuracy: 0.8439 - val_loss: 0.8461 - val_precision: 0.7215 - val_recall: 0.6810 - val_f1: 0.7004 - val_accuracy: 0.7049\n","Epoch 23/100\n","\n","Epoch 00023: val_accuracy improved from 0.83430 to 0.85029, saving model to breastnet_strat.hdf5\n","390/390 - 273s - loss: 0.9667 - precision: 0.8563 - recall: 0.8413 - f1: 0.8486 - accuracy: 0.8503 - val_loss: 0.3809 - val_precision: 0.8542 - val_recall: 0.8474 - val_f1: 0.8508 - val_accuracy: 0.8503\n","Epoch 24/100\n","\n","Epoch 00024: val_accuracy did not improve from 0.85029\n","390/390 - 273s - loss: 1.0446 - precision: 0.8622 - recall: 0.8453 - f1: 0.8535 - accuracy: 0.8543 - val_loss: 1.9389 - val_precision: 0.4193 - val_recall: 0.3990 - val_f1: 0.4087 - val_accuracy: 0.4157\n","Epoch 25/100\n","\n","Epoch 00025: val_accuracy did not improve from 0.85029\n","390/390 - 272s - loss: 0.9567 - precision: 0.8586 - recall: 0.8430 - f1: 0.8506 - accuracy: 0.8520 - val_loss: 0.4064 - val_precision: 0.8393 - val_recall: 0.8336 - val_f1: 0.8364 - val_accuracy: 0.8365\n","Epoch 26/100\n","\n","Epoch 00026: val_accuracy did not improve from 0.85029\n","390/390 - 274s - loss: 0.9302 - precision: 0.8643 - recall: 0.8511 - f1: 0.8576 - accuracy: 0.8576 - val_loss: 0.8861 - val_precision: 0.5183 - val_recall: 0.4658 - val_f1: 0.4904 - val_accuracy: 0.5058\n","Epoch 27/100\n","\n","Epoch 00027: val_accuracy did not improve from 0.85029\n","390/390 - 272s - loss: 0.8741 - precision: 0.8697 - recall: 0.8571 - f1: 0.8633 - accuracy: 0.8650 - val_loss: 0.4500 - val_precision: 0.8274 - val_recall: 0.8074 - val_f1: 0.8172 - val_accuracy: 0.8161\n","Epoch 28/100\n","\n","Epoch 00028: val_accuracy did not improve from 0.85029\n","390/390 - 273s - loss: 0.8762 - precision: 0.8690 - recall: 0.8591 - f1: 0.8640 - accuracy: 0.8653 - val_loss: 0.8879 - val_precision: 0.5860 - val_recall: 0.5734 - val_f1: 0.5795 - val_accuracy: 0.5792\n","Epoch 29/100\n","\n","Epoch 00029: val_accuracy did not improve from 0.85029\n","390/390 - 272s - loss: 0.8564 - precision: 0.8692 - recall: 0.8596 - f1: 0.8643 - accuracy: 0.8647 - val_loss: 2.2190 - val_precision: 0.3342 - val_recall: 0.3212 - val_f1: 0.3275 - val_accuracy: 0.3299\n","Epoch 30/100\n","\n","Epoch 00030: val_accuracy did not improve from 0.85029\n","390/390 - 274s - loss: 0.7981 - precision: 0.8750 - recall: 0.8673 - f1: 0.8711 - accuracy: 0.8713 - val_loss: 0.7859 - val_precision: 0.6911 - val_recall: 0.6701 - val_f1: 0.6802 - val_accuracy: 0.6810\n","Epoch 31/100\n","\n","Epoch 00031: val_accuracy did not improve from 0.85029\n","390/390 - 272s - loss: 0.8076 - precision: 0.8817 - recall: 0.8732 - f1: 0.8774 - accuracy: 0.8779 - val_loss: 0.4039 - val_precision: 0.8431 - val_recall: 0.8394 - val_f1: 0.8412 - val_accuracy: 0.8401\n","Epoch 32/100\n","\n","Epoch 00032: val_accuracy improved from 0.85029 to 0.86555, saving model to breastnet_strat.hdf5\n","390/390 - 274s - loss: 0.7951 - precision: 0.8816 - recall: 0.8717 - f1: 0.8766 - accuracy: 0.8760 - val_loss: 0.3848 - val_precision: 0.8668 - val_recall: 0.8648 - val_f1: 0.8658 - val_accuracy: 0.8656\n","Epoch 33/100\n","\n","Epoch 00033: val_accuracy did not improve from 0.86555\n","390/390 - 272s - loss: 0.7866 - precision: 0.8824 - recall: 0.8767 - f1: 0.8795 - accuracy: 0.8801 - val_loss: 0.4265 - val_precision: 0.8240 - val_recall: 0.8212 - val_f1: 0.8226 - val_accuracy: 0.8234\n","Epoch 34/100\n","\n","Epoch 00034: val_accuracy did not improve from 0.86555\n","390/390 - 273s - loss: 0.7204 - precision: 0.8838 - recall: 0.8780 - f1: 0.8808 - accuracy: 0.8807 - val_loss: 0.5052 - val_precision: 0.8239 - val_recall: 0.8031 - val_f1: 0.8132 - val_accuracy: 0.8169\n","Epoch 35/100\n","\n","Epoch 00035: val_accuracy did not improve from 0.86555\n","390/390 - 272s - loss: 0.7789 - precision: 0.8829 - recall: 0.8766 - f1: 0.8797 - accuracy: 0.8797 - val_loss: 1.6520 - val_precision: 0.6176 - val_recall: 0.6163 - val_f1: 0.6169 - val_accuracy: 0.6170\n","Epoch 36/100\n","\n","Epoch 00036: val_accuracy did not improve from 0.86555\n","390/390 - 273s - loss: 0.7361 - precision: 0.8916 - recall: 0.8853 - f1: 0.8884 - accuracy: 0.8883 - val_loss: 0.4339 - val_precision: 0.8439 - val_recall: 0.8299 - val_f1: 0.8368 - val_accuracy: 0.8387\n","Epoch 37/100\n","\n","Epoch 00037: val_accuracy improved from 0.86555 to 0.88881, saving model to breastnet_strat.hdf5\n","390/390 - 273s - loss: 0.7077 - precision: 0.8909 - recall: 0.8863 - f1: 0.8886 - accuracy: 0.8893 - val_loss: 0.2905 - val_precision: 0.8891 - val_recall: 0.8866 - val_f1: 0.8879 - val_accuracy: 0.8888\n","Epoch 38/100\n","\n","Epoch 00038: val_accuracy did not improve from 0.88881\n","390/390 - 273s - loss: 0.6646 - precision: 0.8928 - recall: 0.8883 - f1: 0.8905 - accuracy: 0.8901 - val_loss: 0.3187 - val_precision: 0.8882 - val_recall: 0.8794 - val_f1: 0.8837 - val_accuracy: 0.8837\n","Epoch 39/100\n","\n","Epoch 00039: val_accuracy did not improve from 0.88881\n","390/390 - 273s - loss: 0.8104 - precision: 0.8868 - recall: 0.8819 - f1: 0.8843 - accuracy: 0.8842 - val_loss: 0.6827 - val_precision: 0.7911 - val_recall: 0.7900 - val_f1: 0.7905 - val_accuracy: 0.7907\n","Epoch 40/100\n","\n","Epoch 00040: val_accuracy did not improve from 0.88881\n","390/390 - 274s - loss: 0.7275 - precision: 0.8922 - recall: 0.8875 - f1: 0.8898 - accuracy: 0.8901 - val_loss: 0.4515 - val_precision: 0.8292 - val_recall: 0.8263 - val_f1: 0.8277 - val_accuracy: 0.8263\n","Epoch 41/100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sHW788TP3IGp","colab_type":"code","colab":{}},"source":["test_datagen=ImageDataGenerator(rescale=1./255)\n","\n","test_generator=test_datagen.flow_from_dataframe(\n","  dataframe=testdf,\n","  directory=path_test,\n","  x_col='Name',\n","  y_col=None,\n","  batch_size=bs,\n","  shuffle=False, #DO NOT SHUFFLE, otherwise you won't be able to calculate monitors afterward \n","  class_mode=None,\n","  target_size=(size,size))\n","\n","\n","breastnet_prediction = []\n","\n","\n","model_bn = create_model(lr, epochs, STEP_SIZE_TRAIN, STEP_SIZE_VALID)\n","\n","model_bn.load_weights(model_name)\n","\n","test_generator.reset()\n","pred = model_bn.predict_generator(\n","        generator=test_generator,\n","        # steps = test_generator.n//test_generator.batch_size,\n","        verbose=1\n","    )\n","\n","class_guess=np.argmax(pred, axis=1)\n","\n","# adapted from COVIDX-Fastai-XResNet18.ipynb\n","# Convert dataframe test labels to list\n","gt = testdf['Class'].tolist()\n","\n","# Convert from label names to class index values (0, 1, 2)\n","from sklearn import preprocessing\n","from sklearn.metrics import confusion_matrix\n","labels = ['COVID-19', 'normal', 'pneumonia']\n","le = preprocessing.LabelEncoder()\n","targets = le.fit_transform(labels)\n","test_preds = le.transform( gt )\n","\n","print(\"Size of class_guess : \" + str(np.size(class_guess)))\n","print(\"Size of test_preds : \" + str(np.size(np.asarray( test_preds ))))\n","print(\"class_guess : \" + str(class_guess))\n","print(\"test_preds : \" + str(test_preds))\n","\n","# Calculate accuracy\n","from sklearn.metrics import accuracy_score\n","acc = accuracy_score( class_guess, np.asarray( test_preds ) )\n","print( \"Accuracy = \" + str( acc ) )\n","\n","# Calculate precision per class\n","print(labels)\n","\n","from sklearn.metrics import precision_score\n","prec = precision_score( class_guess, np.asarray( test_preds ), average=None )\n","print( \"Precision (Positive Predictive Value) per class = \" + str( prec ))\n","\n","# Calculate recall per class\n","from sklearn.metrics import recall_score\n","rec = recall_score(class_guess, np.asarray(test_preds), average=None )\n","print( \"Recall (Sensitiviy) per class = \" + str( rec ))\n","\n","\n","print('Confusion Matrix')\n","print(confusion_matrix(np.asarray( test_preds ), class_guess))\n"],"execution_count":null,"outputs":[]}]}